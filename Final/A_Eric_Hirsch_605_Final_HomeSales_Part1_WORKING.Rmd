---
title: "Untitled"
author: "Eric Hirsch"
date: "11/14/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Load libraries
library(tidymodels)
library(vip)
library(tidyverse)
library(lmtest)
library(skimr)
library(mltools)
library(psych)
```

```{r}
home_sales <- read.csv("D:\\RStudio\\605_Final\\Final\\housing\\train.csv", stringsAsFactors=TRUE, header = TRUE)
home_kaggle_test <- read.csv("D:\\RStudio\\605_Final\\Final\\housing\\test.csv", stringsAsFactors=TRUE, header = TRUE)
home_submit <- read.csv("D:\\RStudio\\605_Final\\Final\\housing\\sample_submission.csv", header = TRUE)
```

#### 1a. Descriptive and Inferential Statistics. Provide univariate descriptive statistics and appropriate plots for the training data set.

```{r}
dim(home_sales)
summary(home_sales)
skim(home_sales)
str(home_sales)
```

Allthough we could do plots on all of the variables, here we will just focus on those numeric variables most correlated with the dependent variable.

```{r}
library(tibble)
home_sales_num <- home_sales %>% select_if(is.numeric)
dfCorr <- as.data.frame(cor(home_sales_num, home_sales_num$SalePrice)) %>%
  tibble::rownames_to_column("Variables") %>% 
  arrange(desc(abs(V1)))

head(dfCorr, 10)

```


```{r}
dfSelectedCols <- home_sales %>%
  dplyr::select(SalePrice, OverallQual, GrLivArea, GarageCars, GarageArea, TotalBsmtSF, X1stFlrSF, FullBath, TotRmsAbvGrd, YearBuilt)

library(ResourceSelection)
kdepairs(dfSelectedCols)
```
Here we can see that there is significant multicollinearity among the variables.  Also, many are skewed right (meaning the distributiion is mainly to the left of the mean.)

#### 1b. Provide a scatterplot matrix for at least two of the independent variables and the dependent variable. 

```{r}
ggplot(home_sales, aes(y=SalePrice, x=GrLivArea, color = X1stFlrSF)) +
  geom_point() +
    stat_smooth(method = "lm", se = FALSE)

ggplot(home_sales, aes(y=SalePrice, x=GarageArea)) +
  geom_point() +
    stat_smooth(method = "lm", se = FALSE)

```

Here we can see that GrLivArea, X1stFlrSF and GarageArea are all correlated with SalePrice. However, they show cone-shaped patterns which might suggest issues with heteroskedasticity, since there is more variation as values get larger.

#### 1c. Derive a correlation matrix for any three quantitative variables in the dataset. Test the hypotheses that the correlations between each pairwise set of variables is 0 and provide an 80% confidence interval. Discuss the meaning of your analysis. Would you be worried about familywise error? Why or why not? 5 point

```{r}
dfSelectedCols2 <- home_sales %>%
  dplyr::select(X1stFlrSF, GrLivArea, YearBuilt)

corMat <- cor(dfSelectedCols2)
corMat
```
Here we see that all of the variables are correlated but particularly X1sFlrSF and GrLivArea.  Looking at the p values:  

```{r}

library(Hmisc)
res <- rcorr(as.matrix(dfSelectedCols2)) 
round(res$P, 3)

```
```{r}
#confidence interval X1stFlrSF vsGrLivArea
ct = cor.test(dfSelectedCols2$X1stFlrSF, dfSelectedCols2$GrLivArea, method="pearson", conf.level=.8)

ct$conf.int[1:2]  
```

```{r}
#confidence interval X1stFlrSF vs YearBuilt
ct = cor.test(dfSelectedCols2$X1stFlrSF,dfSelectedCols2$YearBuilt, method="pearson", conf.level=.8)

ct$conf.int[1:2]  

```

```{r}
#confidence interval GeLivArea vs YearBuilt
ct = cor.test(dfSelectedCols2$GrLivArea,dfSelectedCols2$YearBuilt, method="pearson", conf.level=.8)

ct$conf.int[1:2]  

```


In fact, all of the p values are 0. Confidence intervals are about +1 .03.  

Family-wise error:The Family-wise error rate = $$1 – (1-α)^n$$ where 
α: The significance level for a single hypothesis test
n: The total number of tests

```{r}
alpha=.2
num=3
fwe <- 1 - (1-alpha)^num
fwe
```
The probability of getting a type I error on at least one of the hypothesis tests is nearly 50% so I would be concerned about family-wise error.

#### 2. Invert your correlation matrix from above. (This is known as the precision matrix and contains variance inflation factors on the diagonal.) Multiply the correlation matrix by the precision matrix, and then multiply the precision matrix by the correlation matrix. Conduct LU decomposition on the matrix. 5 points


```{r}
precMat <- solve(corMat)
precMat
```

Surprisingly, the VIFs are not too high.

```{r}

cByp <- corMat %*% precMat
pByc <- precMat %*% corMat

library(matrixcalc)
cByp_LU <- lu.decomposition(cByp)
pByc_LU <- lu.decomposition(pByc)

print(cByp_LU)
print(pByc_LU)
```

#### 3a. Select a variable in the Kaggle.com training dataset that is skewed to the right, shift it so that the minimum value is absolutely above zero if necessary. Then load the MASS package and run fitdistr to fit an exponential probability density function. 

I will be using SalePrice, which we know is skewed from the analysis at the beginning of the exercise.

```{r}
hist(home_sales$SalePrice)
```

#### 3.b Find the optimal value of λ for this distribution, and then take 1000 samples from this exponential distribution using this value (e.g., rexp(1000, λ)).

We fit an exponential pdf, find the optimal lamda, and take 1000 samples.
```{r}

library(MASS)

fit1 <- fitdistr(home_sales$SalePrice, "exponential")
fit1
```
```{r}
set.seed(1)
simdata = rexp(n = 1000, rate = fit1$estimate )
matrixdata =  matrix(simdata, nrow = 1000)
means.exp = apply(matrixdata, 1, mean)
```

#### 3.c Plot a histogram and compare it with a histogram of your original variable. Using the exponential pdf, find the 5th and 95th percentiles using the cumulative distribution function (CDF). Also generate a 95% confidence interval from the empirical data, assuming normality. Finally, provide the empirical 5th percentile and 95th percentile of the data. Discuss. 10 points

```{r}
par(mfcol=c(1,2), mar=c(1,1,2,1))

hist(home_sales$SalePrice)
hist(means.exp)

```

Quantiles for the generated distribution

```{r}
lambda <- 5.527268e-06
prob <- 0.05
# compute the quantile for Exponential dist
qexp(prob,rate=lambda)
```
```{r}
prob <- 0.95
# compute the quantile for Exponential dist
qexp(prob,rate=lambda)
```

Confidence interval for the mean of the empirical distribution, assuming normality

```{r}

t.test(home_sales$SalePrice)

```
Quantiles for the empirical distribution

```{r}

quantile(home_sales$SalePrice, probs = c(0.05, 0.95))
```
We can see how close the generated function is to the actual distribution for SalePrice from the histograms. The quantiles show some differences, however. Quantiles for the generated distribution (9280, 541991) are wider than those for the empirical distribution (88000,326100).  This is perhaps picking up the fact that the empirical distribution is more normal than the generated distribution, in that there are a number of observations before the distribution peaks. The empirical mean CI (176k to 185k) is far closer to the 5th than the 95th quantile - this shows us the skew of the distribution.

## Part II



#### 3. Wrangle

a. Get rid of columns that have na

We would normally investigate whether valuable data would be lost with this step - however, we can see from the above that most variables are preserved and in the interests of time will proceed.
```{r}

home_sales2 <- home_sales %>%
  dplyr::select_if(~ !any(is.na(.)))

```

b. Remove utilities because it only has one value and ID because it is irrelevant

```{r}

home_sales3 <- home_sales2 %>%
  dplyr::select(-Utilities) %>%
  dplyr::select(-Id)

```

c. Scale the numeric variables.

```{r}
numeric_data <- home_sales3 %>% select_if(is.numeric)
nonnumeric_data <- home_sales3 %>% select_if(Negate(is.numeric))

df.scaled <- as.data.frame(scale(numeric_data))
df.scaled$SalePrice <- numeric_data$SalePrice

```

d. Eliminate unbalanced character variables and then one-hot

```{r}
nonnumeric_data2 <- nonnumeric_data %>%
  dplyr::select(ExterQual, Exterior1st, HouseStyle, Neighborhood, LotConfig, LotShape, KitchenQual, HeatingQC, Foundation)

nonnumeric_data3 <- one_hot(as.data.table(nonnumeric_data2), dropCols=TRUE)

home_sales4 <- cbind(nonnumeric_data3, df.scaled)
```

d. Take the log of SalePrice - based on the analysis above, the analysis would benefit from this

```{r}
home_sales_wrangled <- home_sales4 %>%
  mutate(SalePrice = log(SalePrice)) 
```

#### Choosing variables for a base model

We don't want to choose all of the variables as this would inevitably lead to multicollinearity, reduction in adjusted R squared and serious overfitting.  

```{r}

head(dfCorr, 20)

```
Based on this analysis, we'll start with the following numeric variables and then add some categorical variable.  We choose based on correlation and not collinear with other obvious variables (e.g. garage area and garage cars):


```{r}

mod_1 <- lm(SalePrice ~ LotArea + YearBuilt + YearRemodAdd + TotalBsmtSF + X1stFlrSF + GrLivArea + GarageCars + WoodDeckSF + OpenPorchSF + TotRmsAbvGrd + Fireplaces + FullBath, data = home_sales_wrangled)
summary(mod_1)

```

We do the same analysis with categorical variables.

```{r}
nonnumeric_data4 <- cbind(nonnumeric_data3, df.scaled$SalePrice)
dfCor <- as.data.frame(cor(nonnumeric_data4, nonnumeric_data4$V2))
dfCor
```

Our Final model 
```{r}

home_sales_wrangled_Final <- home_sales_wrangled %>%
  dplyr::select(SalePrice, LotArea, YearBuilt, YearRemodAdd, TotalBsmtSF, X1stFlrSF, GrLivArea, GarageCars, WoodDeckSF, OpenPorchSF, TotRmsAbvGrd, Fireplaces, FullBath, -LotConfig_Corner:LotConfig_Inside, LotShape_IR1:LotShape_Reg)


mod_2 <- lm(SalePrice ~ ., data = home_sales_wrangled_Final)
summary(mod_2)
```
We now step through the model to see the effect on adjusted R squared of eliminating variables.  We do this with the R Step_AIC function.

```{r}
library(MASS)
step1 <- stepAIC(mod_2, trace=FALSE)
summary(step1)

```
```{r}
plot(step1)
```


Thus our final df:

```{r}

dfFinal <- step1$model
head(dfFinal)

```

Load test data and prepare as we did the training data

```{r}

numeric_data_test <- home_kaggle_test %>% select_if(is.numeric)
nonnumeric_data_test <- home_kaggle_test %>% select_if(Negate(is.numeric))

#1. one-hot the categorical variables

library(data.table)
library(tidymodels)

nonnumeric_data_test1 <- nonnumeric_data_test %>%
  dplyr::select(ExterQual, Exterior1st, HouseStyle, Neighborhood, KitchenQual, HeatingQC, Foundation)

nonnumeric_data_OneHot <- one_hot(data.table::as.data.table(nonnumeric_data_test1), dropCols=TRUE)

#Scale the numeric variables

df.scaled_test <- as.data.frame(scale(numeric_data_test))
df.scaled_test$Id <- numeric_data_test$Id

#recombine

dfTest_Final <- cbind(df.scaled_test, nonnumeric_data_OneHot)
```

#Check if the dataset is missing columns from the test dataset and if so drop them from the original
```{r}

library(janitor)
compare_df_cols(dfTest_Final, dfFinal)

dfFinal1 <- dfFinal %>%
  dplyr::select(-LotConfig_CulDSac) %>%
  dplyr::select(-LotConfig_FR2)  %>%
  dplyr::select(-LotShape_IR3)
  
```

Run the model

```{r}

mod_3 <- lm(SalePrice ~ ., data = dfFinal1)
summary(mod_3)

step2 <- stepAIC(mod_3, trace=FALSE)
summary(step2)
```

```{r}

plot(step2)

```
Remove outliers

```{r}
dfFinal2 <- slice(dfFinal1, -1299)
dfFinal2 <- slice(dfFinal2, -524)
dfFinal2 <- slice(dfFinal2, -1001)
dfFinal2 <- slice(dfFinal2, -31)
```

```{r}

mod_4 <- lm(SalePrice ~ ., data = dfFinal2)
summary(mod_4)

step3 <- stepAIC(mod_4, trace=FALSE)
summary(step3)
```

Transform GrLivArea

```{r}
hist(log(dfFinal1$GrLivArea))

dfFinal3 <- dfFinal1 %>%
  mutate(GrLivArea = log(GrLivArea+10))

mod_5 <- lm(SalePrice ~ ., data = dfFinal3)
summary(mod_5)

step4 <- stepAIC(mod_5, trace=FALSE)
summary(step4)
```

Predict the model
```{r}

predictions <- predict(step2,newdata=dfTest_Final)
predictions <- data.frame(as.vector(predictions))
predictions$Id <- dfTest_Final$Id
predictions[,c(1,2)] <- predictions[,c(2,1)]
colnames(predictions) <- c("Id", "SalePrice")
predictions[is.na(predictions)] <- log(mean(home_sales$SalePrice))
predictions$SalePrice <- exp(predictions$SalePrice)
head(predictions)

```

```{r}

#write_csv(predictions, "C:\\Users\\eric.hirsch\\Desktop\\people8.csv")
```

Removing outliers is not an improvement (.17020)  Neither is using the mean (.16984)  So we stick with step2, the original model. Transforming GrLivArea also hurts the model (.23417)  Best model is step 2.
