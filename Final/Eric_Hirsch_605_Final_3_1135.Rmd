---
title: "Untitled"
author: "Eric Hirsch"
date: "11/14/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Load libraries
library(tidymodels)
library(vip)
library(tidyverse)
library(lmtest)
library(skimr)
library(mltools)
library(psych)
```

```{r}
home_sales <- read.csv("D:\\RStudio\\605_Final\\Final\\housing\\train.csv", stringsAsFactors=TRUE, header = TRUE)
home_kaggle_test <- read.csv("D:\\RStudio\\605_Final\\Final\\housing\\test.csv", stringsAsFactors=TRUE, header = TRUE)
home_submit <- read.csv("D:\\RStudio\\605_Final\\Final\\housing\\sample_submission.csv", header = TRUE)
```
```{r}

head(home_sales, 10)
dim(home_sales)
glimpse(home_sales)
summary(home_sales)
skim(home_sales)
```


```{r}
dfSelectedCols <- home_sales %>%
  dplyr::select(SalePrice, GrLivArea, TotRmsAbvGrd, YearBuilt, YrSold)

library(ResourceSelection)
kdepairs(dfSelectedCols)
```

```{r}
ggplot(home_sales, aes(y=SalePrice, x=LotArea, color = GarageArea)) +
  geom_point() +
    stat_smooth(method = "lm", se = FALSE)

ggplot(home_sales, aes(y=SalePrice, x=GarageArea)) +
  geom_point() +
    stat_smooth(method = "lm", se = FALSE)
#would need to scale to do 2 on one

```

```{r}
dfSelectedCols2 <- home_sales %>%
  dplyr::select(SalePrice, GrLivArea, YrSold)

corMat <- cor(dfSelectedCols2)
corMat
```

```{r}

library(Hmisc)
res <- rcorr(as.matrix(dfSelectedCols2)) # rcorr() accepts matrices only

# display p-values (rounded to 3 decimals)
round(res$P, 3)

```
Family-wise error rate = $$1 – (1-α)^n$$ where 
α: The significance level for a single hypothesis test
n: The total number of tests

```{r}
alpha=.2
num=3
fwe <- 1 - (1-alpha)^num
fwe
```
The probability of getting a type I error on at least one of the hypothesis tests is nearly 50% so I would be concerned about familywise error.

```{r}
precMat <- solve(corMat)
precMat
```
```{r}

cByp <- corMat %*% precMat
pByc <- precMat %*% corMat

library(matrixcalc)
cByp_LU <- lu.decomposition(cByp)
pByc_LU <- lu.decomposition(pByc)
```
```{r}
L <- cByp_LU$L
U <- cByp_LU$U
print( L )
print( U )
print( L %*% U )
print( cByp )
```


```{r}
L <- pByc_LU$L
U <- pByc_LU$U
print( L )
print( U )
print( L %*% U )
print( pByc )
```

```{r}
hist(home_sales$SalePrice)

```
```{r}

library(MASS)

fit1 <- fitdistr(home_sales$SalePrice, "exponential")
fit1
```
```{r}
set.seed(1)
simdata = rexp(n = 1000, rate = 5.527268e-06 )
matrixdata =  matrix(simdata, nrow = 1000)
means.exp = apply(matrixdata, 1, mean)

par(mfcol=c(1,2), mar=c(1,1,2,1))

hist(home_sales$SalePrice)
hist(means.exp)

```
```{r}
lambda <- 5.527268e-06
prob <- 0.05
# compute the quantile for Exponential  dist
qexp(prob,rate=lambda)
```
```{r}
prob <- 0.95
# compute the quantile for Exponential  dist
qexp(prob,rate=lambda)
```

```{r}

t.test(home_sales$SalePrice)

```
```{r}

quantile(home_sales$SalePrice, probs = c(0.05, 0.95))
```
#### Build a regression model

```{r}
m1 <- lm(SalePrice ~ GrLivArea, data=home_sales)
summary(m1)
plot(m1)

```

#### 1. Split the data

a. get rid of columns that are mostly na

```{r}

home_sales2 <- home_sales %>%
  dplyr::select_if(colSums(!is.na(.))>1200) 

```

remove utlities becasue it only has one value

```{r}

home_sales3 <- home_sales2 %>%
  dplyr::select(-Utilities) %>%
  dplyr::select(-Id)

```


```{r}

set.seed(271)

# Create a split object
homes_split <- initial_split(home_sales3, prop = 0.75, 
                             strata = SalePrice)

# Build training data set
homes_training <- homes_split %>% 
                  training()

# Build testing data set
#homes_test <- homes_split %>% 
              #testing()
homes_test <- home_kaggle_test

```
Feature engineering


```{r}
homes_recipe <- recipe(SalePrice ~ ., data = homes_training) %>% 
                step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
                step_normalize(all_numeric(), -all_outcomes()) %>% 
                step_dummy(all_nominal(), - all_outcomes())
```
Test it looks right

```{r}

homes_recipe %>% 
  prep() %>% 
  bake(new_data = homes_test)

```
specify a mode

```{r}
lm_model <- linear_reg() %>% 
            set_engine('lm') %>% 
            set_mode('regression')
```

create a workflow

```{r}
homes_workflow <- workflow() %>% 
                  add_model(lm_model) %>% 
                  add_recipe(homes_recipe)
```

Execute the workflow

```{r}
homes_fit <- homes_workflow %>% 
             last_fit(split = homes_split)
```

Examine metrics
```{r}
homes_fit %>% collect_metrics()

```
Get test prediction

```{r}
# Obtain test set predictions data frame
homes_results <- homes_fit %>% 
                 collect_predictions()
# View results
homes_results
```
Plot r2
```{r}
ggplot(data = homes_results,
       mapping = aes(x = .pred, y = SalePrice)) +
  geom_point(color = '#006EA1', alpha = 0.25) +
  geom_abline(intercept = 0, slope = 1, color = 'orange') +
  labs(title = 'Linear Regression Results - Home Sales Test Set',
       x = 'Predicted Selling Price',
       y = 'Actual Selling Price')
```
Variable importance

```{r}

homes_training_baked <- homes_recipe %>% 
                        prep() %>% 
                        bake(new_data = homes_training)

# View results
homes_training_baked

homes_lm_fit <- lm_model %>% 
                fit(SalePrice ~ ., data = homes_training_baked)

vip(homes_lm_fit)

```
Plots
```{r}
homes_lm_fit <- lm_model %>% 
                fit(SalePrice ~ ., data = homes_training_baked)

homes_fit %>% collect_metrics()
tidy(homes_lm_fit)
glance(homes_lm_fit)

par(mfrow=c(2,2)) # plot all 4 plots in one

plot(homes_lm_fit$fit, 
     pch = 16)
```

```{r}

library("writexl")
#write_xlsx(df, "C:\\Users\\Eric\\Desktop\\people.xlsx")
```


try again with log of y


#### 1. Split the data

Adjustments : 
1. log of y
2. eliminate na rows after eliminating mostly na columns
3. fix one-hots to account for sparse columns


```{r}

set.seed(271)

home_sales4 <- home_sales3 %>%
  mutate(SalePrice = log(SalePrice)) %>%
  na.omit(SalePrice)

# Create a split object
homes_split <- initial_split(home_sales4, prop = 0.75, 
                             strata = SalePrice)

# Build training data set
homes_training <- homes_split %>% 
                  training()

# Build testing data set
homes_test <- homes_split %>% 
              testing()


```
Feature engineering


```{r}
homes_recipe <- recipe(SalePrice ~ ., data = homes_training) %>% 
                step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
                step_normalize(all_numeric(), -all_outcomes()) %>% 
                step_dummy(all_nominal(), - all_outcomes())
```
Test it looks right

```{r}

print(homes_recipe$template)

homes_recipe %>% 
  prep() %>% 
  bake(new_data = homes_test)

```
specify a mode

```{r}
lm_model <- linear_reg() %>% 
            set_engine('lm') %>% 
            set_mode('regression')
```

create a workflow

```{r}
homes_workflow <- workflow() %>% 
                  add_model(lm_model) %>% 
                  add_recipe(homes_recipe)
```

Execute the workflow

```{r}
homes_fit <- homes_workflow %>% 
             last_fit(split = homes_split)
```

Examine metrics
```{r}
homes_fit %>% collect_metrics()

```
Get test prediction

```{r}
# Obtain test set predictions data frame
homes_results <- homes_fit %>% 
                 collect_predictions()
# View results
homes_results
```
Plot r2
```{r}
ggplot(data = homes_results,
       mapping = aes(x = .pred, y = SalePrice)) +
  geom_point(color = '#006EA1', alpha = 0.25) +
  geom_abline(intercept = 0, slope = 1, color = 'orange') +
  labs(title = 'Linear Regression Results - Home Sales Test Set',
       x = 'Predicted Selling Price',
       y = 'Actual Selling Price')
```
Variable importance

```{r}

homes_training_baked <- homes_recipe %>% 
                        prep() %>% 
                        bake(new_data = homes_training)

# View results
homes_training_baked

homes_lm_fit <- lm_model %>% 
                fit(SalePrice ~ ., data = homes_training_baked)

vip(homes_lm_fit)

```
Plots
```{r}
homes_lm_fit <- lm_model %>% 
                fit(SalePrice ~ ., data = homes_training_baked)

homes_fit %>% collect_metrics()
tidy(homes_lm_fit)
glance(homes_lm_fit)
summary(homes_lm_fit$fit)

par(mfrow=c(2,2)) # plot all 4 plots in one

plot(homes_lm_fit$fit, 
     pch = 16)
```

Try again with reduction of columns


```{r}

dfResults <- as.data.frame(tidy(homes_lm_fit)) %>%
  filter(p.value<=.05) %>%
  filter(term != "(Intercept)") %>%
  arrange(desc(estimate))

```
```{r}

vCols <- c("SalePrice", "Exterior2nd", "Condition2", "GarageCond", "MSZoning", "RoofStyle", "Street", "Condition1", "GrLivArea","Neighborhood", "TotalBsmtSF" , "SaleCondition", "BsmtExposure", "LotArea", "YearBuilt", "GarageCars")

home_sales5 <- home_sales4 
  #dplyr::select(vCols)

set.seed(271)


# Create a split object
homes_split <- initial_split(home_sales5, prop = 0.75, 
                             strata = SalePrice)

# Build training data set
homes_training <- homes_split %>% 
                  training()

# Build testing data set
homes_test <- homes_split %>% 
              testing()


```
Feature engineering


```{r}
homes_recipe <- recipe(SalePrice ~ ., data = homes_training) %>% 
                step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
                step_normalize(all_numeric(), -all_outcomes()) %>% 
                step_dummy(all_nominal(), - all_outcomes())
```
Test it looks right

```{r}

homes_recipe %>% 
  prep() %>% 
  bake(new_data = homes_test)

```
specify a mode

```{r}
lm_model <- linear_reg() %>% 
            set_engine('lm') %>% 
            set_mode('regression')
```

create a workflow

```{r}
homes_workflow <- workflow() %>% 
                  add_model(lm_model) %>% 
                  add_recipe(homes_recipe)
```

Execute the workflow

```{r}
homes_fit <- homes_workflow %>% 
             last_fit(split = homes_split)
```

Examine metrics
```{r}
homes_fit %>% collect_metrics()

```
Get test prediction

```{r}
# Obtain test set predictions data frame
homes_results <- homes_fit %>% 
                 collect_predictions()
# View results
homes_results
```
Plot r2
```{r}
ggplot(data = homes_results,
       mapping = aes(x = .pred, y = SalePrice)) +
  geom_point(color = '#006EA1', alpha = 0.25) +
  geom_abline(intercept = 0, slope = 1, color = 'orange') +
  labs(title = 'Linear Regression Results - Home Sales Test Set',
       x = 'Predicted Selling Price',
       y = 'Actual Selling Price')
```
Variable importance

```{r}

homes_training_baked <- homes_recipe %>% 
                        prep() %>% 
                        bake(new_data = homes_training)

# View results
homes_training_baked

homes_lm_fit <- lm_model %>% 
                fit(SalePrice ~ ., data = homes_training_baked)

vip(homes_lm_fit)

```
Plots
```{r}
homes_lm_fit <- lm_model %>% 
                fit(SalePrice ~ ., data = homes_training_baked)

homes_fit %>% collect_metrics()

tidy(homes_lm_fit)
glance(homes_lm_fit)
summary(homes_lm_fit$fit)

par(mfrow=c(2,2)) # plot all 4 plots in one

plot(homes_lm_fit$fit, 
     pch = 16)
```

```{r}
#df <- predict(homes_lm_fit, new_data = home_kaggle_test)
#df$.pred[is.na(df$.pred)]<-median(df$.pred,na.rm=TRUE)
#print(df)

#library("writexl")
#write_xlsx(df, "C:\\Users\\Eric\\Desktop\\people.xlsx")


```

Try again without tidymodels but using home_sales4 which already has log and na control

1. scale
2. one-hot
3. run regression
4. predict

```{r}
library(data.table)

home_sales10 <- one_hot(as.data.table(home_sales4), dropCols=TRUE)
home_sales11 <- as.data.frame(scale(home_sales10[-SalePrice]))

home_sales12 <- home_sales10 %>%
  dplyr::select_if(~ !any(is.na(.)))

describe(home_sales12$SalePrice)

```
```{r}

m1 = lm(SalePrice ~ ., data = home_sales12)
summary(m1)

par(mfrow=c(2,2)) # plot all 4 plots in one
plot(m1)

```
drop outliers

```{r}



```
predict
```{r}



home_kaggle_sales2 <- home_kaggle_test %>%
  dplyr::select_if(colSums(!is.na(.))>=1459) 

library(data.table)

home_kagglesales10 <- one_hot(as.data.table(home_kaggle_sales2), dropCols=TRUE)
home_kagglesales11 <- as.data.frame(scale(home_kagglesales10))

home_kagglesales12 <- home_kagglesales11 %>%
  dplyr::select_if(~ !any(is.na(.)))

#describe(home_kagglesales12)

```
```{r}
library(janitor)
cols <- janitor::compare_df_cols(home_sales12, home_kagglesales11)

cols1 <- cols %>%
  dplyr::filter(is.na(home_kagglesales11)) %>%
  dplyr::filter(column_name != "SalePrice")

cols2 <- as.vector(cols1$column_name)
cols2

home_sales13 <- home_sales12 %>%
  dplyr::select(-cols2)

```

```{r}
m2 = lm(SalePrice ~ ., data = home_sales13)
summary(m2)

par(mfrow=c(2,2)) # plot all 4 plots in one
plot(m2)
```
```{r}

prediction <- as.data.frame(predict(m2, newdata = home_kagglesales11))
prediction
prediction <- exp(prediction)
prediction



```

```{r}

#library("writexl")
#write_xlsx(prediction, "C:\\Users\\erico\\Desktop\\people2.xlsx")
```

Deal with overfit

```{r}

df <- as.data.frame(summary(m2)$coefficients[])
df <- cbind(Coeffs = rownames(df), df)
rownames(df) <- 1:nrow(df)

df["p_value"] <- df[,5]

cols3 <- df %>%
  filter(p_value<.05) %>%
  arrange(desc(Estimate^2)) %>%
  dplyr::select(Coeffs) %>%
  filter(Coeffs != "(Intercept)") %>%
  head(200)

cols3[5,1] <- "RoofMatl_Tar&Grv"
#cols3 <-cols3[-7]
vCols1 <- cols3$Coeffs

vCols1 <- c("SalePrice", "OverallCond", "GrLivArea", "OverallQual", "RoofMatl_WdShngl", "Neighborhood_MeadowV", "Neighborhood_Edwards", "YearBuilt", "LotArea")

vCols1 <- c("SalePrice", "OverallCond", "GrLivArea", "OverallQual", "YearBuilt", "LotArea")

home_sales14 <- home_sales13 %>%
  dplyr::select(c(vCols1, SalePrice))

m4 = lm(SalePrice ~ ., data = home_sales14)
summary(m4)

par(mfrow=c(2,2)) # plot all 4 plots in one
plot(m4)

```

```{r}
prediction2 <- as.data.frame(predict(m4, newdata = home_kagglesales11))
prediction2
prediction2 <- exp(prediction2)
prediction2


#library("writexl")
#write_xlsx(prediction2, "C:\\Users\\erico\\Desktop\\people.xlsx")
```