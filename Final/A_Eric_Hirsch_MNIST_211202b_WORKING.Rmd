---
title: "Untitled"
author: "Eric Hirsch"
date: "12/13/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(kernlab)
library(readr)
library(caret)
library(tidyverse)
library(gridExtra)
library(janitor)
library(Matrix)
```

#### 1. Using the training.csv file, plot representations of the first 10 images to understand the data format. Go ahead and divide all pixels by 255 to produce values between 0 and 1. (This is equivalent to min-max scaling.) (5 points)

a. Read Data

```{r}

location = "work"

if (location=="work")
{
  mnist_raw <- read.csv("C:\\Users\\eric.hirsch\\Desktop\\RStudio\\605_Final\\Final\\MNIST.csv",header=TRUE)
} else if (loccation=="laptop")
  
{
  mnist_raw <- read.csv("D:\\RStudio\\CUNY_605\\Final\\MNIST.csv",header=TRUE)
} else
  
{
  mnist_raw <- read.csv("D:\\RStudio\\605_Final\\Final\\MNIST.csv",header=TRUE)
}

```

b. Scale Table
```{r}

mnist <- mnist_raw
colnames(mnist)[1]<-"Digit"
mnist[is.na(mnist)] <- 0

mnist_sample <- mnist[sample(nrow(mnist), 5000), ]
mnist_norm<-as.matrix(mnist[,-1]/255)

```

c. Plot the first 10 images

```{r}

par(mfcol=c(1,5), mar=c(1,1,2,1))

for (i in 1:10)
{
im2<-matrix((mnist[i,2:ncol(mnist)]), nrow=28, ncol=28) 
im_numbers <- apply(im2, 2, as.numeric)
image(1:28, 1:28, im_numbers, col=gray((0:255)/255))
}

```

#### 2. What is the frequency distribution of the numbers in the dataset? (5 points)

```{r}

tabyl(mnist, Digit)

# Don't map a variable to y
ggplot(mnist, aes(x=factor(Digit)))+
  geom_bar(stat="count")
```

The digits are fairly uniformly distributed, with 1s being the most represented (11.1%) and 5s the least (9.0%).

#### 3. For each number, provide the mean pixel intensity. What does this tell you? (5 points)

```{r}

dfMeans <- mnist
dfMeans$Mean <- apply(dfMeans[,2:785], 1, mean)

dfMeans2 <- dfMeans %>%
  group_by(Digit) %>%
  summarize(NumMean = mean(Mean))
dfMeans2
```

This tells us the relative space a number occupies within its matrix.  We can see that the number 1, a simple line, occupies the least space. The seven, which is a line with a small attached segment, occupies somewhat more space.  In contrast, the intensity of zero is more than twice that of 1, suggesting it occupies a lot of space. When we finish the analysis, it would be interesting to see whether there is more confusion between digits that occupy similar amounts of space.

#### 4. Reduce the data by using principal components that account for 95% of the variance. How many components did you generate? Use PCA to generate all possible components (100% of the variance). How many components are possible? Why? (5 points) 

```{r}

#Perform PCA on the scaled matrix
mnist_norm_cov <- cov(mnist_norm)
pca <- prcomp(mnist_norm_cov)
```

```{r}

# Show the cumulative variance
variance_explained <- as.data.frame(pca$sdev^2/sum(pca$sdev^2))

variance_explained <- cbind(c(1:784),variance_explained,cumsum(variance_explained[,1]))

colnames(variance_explained) <- c("n","individual_var_explained","cumulative_var_explained")

head(variance_explained,20)

ggplot(variance_explained, aes(n, cumulative_var_explained)) +
  geom_point()
```

I needed 20 components to account for 95% of the variance.  

At 536 components, additional components no longer increase the variance explained. As for the total components actually possible, there would be 784 - unless there is collinearity, in which case there would be fewer.

#### 5. Plot the first 10 images generated by PCA. They will appear to be noise. Why? (5 points)

```{r}

# reconstruct matrix
restr <- pca$x[,1:20] %*% t(pca$rotation[,1:20])

# unscale and uncenter the data
if(pca$scale != FALSE){
  restr <- scale(restr, center = FALSE , scale=1/pca$scale)
}
if(all(pca$center != FALSE)){
  restr <- scale(restr, center = -1 * pca$center, scale=FALSE)
}

par(mfcol=c(2,5), mar=c(1,1,2,1))

for (i in 1:10)
{

rst <- matrix(data=rev(restr[i,]), nrow=28, ncol=28)
image(1:28, 1:28, rst, col=gray((0:255)/255))

}

```

PVC does not ccount for digit groupings, so these images are a mix of all digits - they capture the sort of average uber digit, which mystically enough looks like a zero.

#### 6. Now, select only those images that have labels that are 8â€™s. Re-run PCA that accounts for all of the variance (100%). Plot the first 10 images. What do you see? (5 points)

Now if we filter just for the digit 8:

```{r}

mnist8 <- mnist %>%
  filter(Digit==8)

mnist_norm8<-as.matrix(mnist8[,-1]/255)

mnist_norm_cov8 <- cov(mnist_norm8)
pca8 <- prcomp(mnist_norm_cov8)
```

```{r}

restr <- pca8$x[,1:784] %*% t(pca8$rotation[,1:784])

par(mfcol=c(2,5), mar=c(1,1,2,1))

for (i in 1:10)
{

im2<-matrix((mnist8[i,2:ncol(mnist8)]), nrow=28, ncol=28) 
im_numbers <- apply(im2, 2, as.numeric)
image(1:28, 1:28, im_numbers, col=gray((0:255)/255))

rst <- matrix(data=rev(restr[i,]), nrow=28, ncol=28)
image(1:28, 1:28, rst, col=gray((0:255)/255))

}

```

This does a much better job of capturing the images in the dataset, because 

#### 7. Build a multinomial model on the entirety of the training set. Then provide its classification accuracy (percent correctly identified) as well as a matrix of observed versus forecast values (confusion matrix). This matrix will be a 10 x 10, and correct classifications will be on the diagonal. (10 points)

This is the model:

```{r}

# Load the nnet package
require(nnet)

mnist_raw[is.na(mnist_raw)] <- 0
mnist_scaled1 <- as.data.frame(mnist_raw/255)
mnist_scaled1[,1] <- mnist_raw[,1]
colnames(mnist_scaled1)[1]<-"Digit"

```
```{r}
# Train the model
#m_1 <- multinom(Digit ~ ., data=mnist_scaled1, MaxNWts =1000000, maxit=1000)

m_1 <- multinom(Digit ~ ., data=mnist_scaled1, MaxNWts =1000000, maxit=10)
```

```{r}
#Make predictions
training_pred <- predict(m_1, mnist_scaled1)
```

This is the confusion matrix for the model

```{r}
cm <- table(training_pred, mnist_scaled1$Digit)
cm
```

This is the accuracy for the model
```{r}
sum(diag(cm))/sum(cm)

```
